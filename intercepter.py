import asyncio
import json
import os
import random
import re
import time
from pathlib import Path
from playwright.async_api import async_playwright, Playwright, TimeoutError, Response

class Config:
    """Configuration constants for the scraper."""
    RANK_URL = "https://buyin.jinritemai.com/dashboard/merch-picking-hall/rank?btm_ppre=a10091.b24215.c68160.d839440_i16852609794&btm_pre=a10091.b71710.c68160.d839440_i1482933506&btm_show_id=ebf82770-5d02-48ed-9f92-c1a75d6fd2f3&pre_universal_page_params_id=&universal_page_params_id=e0b97666-5e57-41ab-850a-5418bb06acad"
    RANK_API_URL_PART = "pc/leaderboard/center/pmt"
    DETAIL_API_URL_PART = "pc/selection/decision/pack_detail"
    DETAIL_PAGE_URL_TEMPLATE = "https://buyin.jinritemai.com/dashboard/merch-picking-library/merch-promoting?id={}"
    STORAGE_STATE_FILE = Path("storage_state.json")
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
    LOGIN_TIMEOUT = 300000  # 5 minutes
    REQUEST_TIMEOUT = 30000  # 30 seconds

INIT_SCRIPT = """
    Object.defineProperty(navigator, 'webdriver', {get: () => false});
    Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en', 'en-GB']});
    Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
    const originalQuery = window.navigator.permissions.query;
    window.navigator.permissions.query = (parameters) => (
        parameters.name === 'notifications' ?
        Promise.resolve({ state: Notification.permission }) :
        originalQuery(parameters)
    );
    try {
        const getParameter = WebGLRenderingContext.prototype.getParameter;
        WebGLRenderingContext.prototype.getParameter = function(parameter) {
          if (parameter === 37445) { # VENDOR
            return 'Intel Open Source Technology Center';
          }
          if (parameter === 37446) { # RENDERER
            return 'Mesa DRI Intel(R) Ivybridge Mobile ';
          }
          return getParameter(parameter);
        };
    } catch (e) {
        console.log('Could not spoof WebGL');
    }
"""

def _is_rank_data_response(response: Response) -> bool:
    """Check if the response is for the rank data API."""
    return Config.RANK_API_URL_PART in response.url

def _is_detail_core_data_response(response: Response) -> bool:
    """Check if the response is for the detail page core data API."""
    if Config.DETAIL_API_URL_PART not in response.url or response.request.method != "POST":
        return False
    try:
        post_data = response.request.post_data_json
        return post_data.get("data_module") == "core"
    except (json.JSONDecodeError, AttributeError):
        return False

def _is_detail_7day_data_response(response: Response) -> bool:
    """Check if the response is for the detail page 7-day data API."""
    if Config.DETAIL_API_URL_PART not in response.url or response.request.method != "POST":
        return False
    try:
        post_data = response.request.post_data_json
        return (
            post_data.get("data_module") == "dynamic" and
            post_data.get("dynamic_params", {}).get("promotion_data_params", {}).get("time_range") == '7'
        )
    except (json.JSONDecodeError, AttributeError):
        return False

async def get_response_json(response: Response, description: str):
    """Safely get JSON from a response and print it."""
    print(f"\nâœ… --- Successfully intercepted {description} ---")
    try:
        data = await response.json()
        # print(json.dumps(data, indent=2, ensure_ascii=False))
        # print(f"--- End of {description} ---")
        return data
    except json.JSONDecodeError:
        print(f"âŒ Could not parse {description} response as JSON.")
        print("Received response text:")
        print(await response.text())
        return None


async def cat_run(page, page_detail, cat, max_count=None, catch_per_minute=3):
    data_list = []
    try:
        async with page.expect_response(_is_rank_data_response, timeout=Config.REQUEST_TIMEOUT) as response_info:
            await page.locator("div").filter(has_text=re.compile(r"^" + cat + "$")).click()

        rank_response = await response_info.value
        rank_data = await get_response_json(rank_response, "Rank Data")

        promotions = rank_data.get("data", {}).get("promotions") if rank_data else []
        if not promotions:
            print("âŒ Could not find 'promotions' in rank data. Cannot proceed.")
            return data_list
        # ä»Šæ—¥çš„æ—¥æœŸ
        today = time.strftime("%Y-%m-%d", time.localtime())
        # å­˜å‚¨åœ°å€
        cache_dir = f"data/{today}/{cat}"
        # ç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»º
        Path(cache_dir).mkdir(parents=True, exist_ok=True)
        # å¾ªç¯è®¿é—®è¯¦æƒ…é¡µ
        for index, item in enumerate(promotions):
            # éšæœºç¡çœ 15-20ç§’
            if max_count is not None and index + 1 > max_count:
                break
            first_product_id = item.get("promotion_id")
            if not first_product_id:
                print("âŒ Could not find 'product_id' for the first product. Cannot proceed.")
                continue
            # å­˜å‚¨æ–‡ä»¶åœ°å€
            file_path = f"{cache_dir}/{first_product_id}.json"
            if os.path.exists(file_path):
                print(f"æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡ï¼š{file_path}")
                continue

            if index != 0:
                # æ¯åˆ†é’ŸæŠ“å–nä¸ªå•†å“
                sleep_time = random.uniform(60 / catch_per_minute - 10, 60 / catch_per_minute + 10)
                print(f"éšæœºç¡çœ ...ç­‰å¾…{sleep_time}s")
                time.sleep(sleep_time)

            detail_page_url = Config.DETAIL_PAGE_URL_TEMPLATE.format(first_product_id)
            print(f"Found product ID: {first_product_id}")
            print(f"Navigating to detail page: {detail_page_url}")

            # --- Fetch Core Data ---
            async with page_detail.expect_response(_is_detail_core_data_response, timeout=Config.REQUEST_TIMEOUT) as core_response_info:
                await page_detail.goto(detail_page_url, wait_until="domcontentloaded")

            core_response = await core_response_info.value
            detail_data = await get_response_json(core_response, "Detail Page Core Data")
            if not detail_data:
                continue

            # --- Fetch 7-Day Data ---
            print("Clicking 'è¿‘7å¤©' (Last 7 days) to get 7-day data...")
            try:
                async with page_detail.expect_response(_is_detail_7day_data_response, timeout=Config.REQUEST_TIMEOUT) as seven_day_response_info:
                    await page_detail.get_by_text("è¿‘7å¤©", exact=True).click()
                seven_day_response = await seven_day_response_info.value
                seven_data = await get_response_json(seven_day_response, "Detail Page 7-Day Data")
                if not seven_data:
                    continue
                save_data = {
                    "rank": index,
                    "category": cat,
                    "detail_data": detail_data,
                    "seven_data": seven_data,
                }
                data_list.append(save_data)
                print("æ•°æ®ä¿å­˜ä¸­...:" + file_path)
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(json.dumps(save_data, indent=4, ensure_ascii=False))
                print("æ•°æ®ä¿å­˜å®Œæ¯•")
            except TimeoutError:
                print(f"âŒ Timed out waiting for 7-day data after clicking 'è¿‘7å¤©'.")
                print("ğŸ’¡ This might happen if the 7-day data was already loaded by default.")
                continue
    except TimeoutError:
        print(f"âŒ Timed out waiting for 7-day data after clicking 'è¿‘7å¤©'.")
        print("ğŸ’¡ This might happen if the 7-day data was already loaded by default.")
    return data_list


async def get_chrome(playwright, mode, remote_config):
    if mode == "remote":
        """Main execution function."""
        # !!! é‡è¦æç¤º !!!
        # 1. è¯·å°†ä¸‹é¢çš„ <YOUR_WINDOWS_USERNAME> æ›¿æ¢ä¸ºæ‚¨çš„å®é™… Windows ç”¨æˆ·åã€‚
        # 2. åœ¨è¿è¡Œè„šæœ¬ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å…³é—­æ‰€æœ‰ Chrome æµè§ˆå™¨å®ä¾‹ã€‚

        user_data_dir = remote_config["user_data_dir"]
        executable_path = remote_config["executable_path"]

        context = await playwright.chromium.launch_persistent_context(
            user_data_dir,
            headless=False,
            executable_path=executable_path,
            user_agent=Config.USER_AGENT,
            # args=['--profile-directory=Default'] # å¦‚æœæ‚¨æœ‰å¤šä¸ªé…ç½®æ–‡ä»¶ï¼Œå¯ä»¥æŒ‡å®šä½¿ç”¨æŸä¸€ä¸ª
        )

        await context.add_init_script(INIT_SCRIPT)
        # æŒä¹…åŒ–ä¸Šä¸‹æ–‡é€šå¸¸ä¼šæœ‰ä¸€ä¸ªé»˜è®¤çš„ç©ºç™½é¡µé¢ï¼Œæˆ‘ä»¬è·å–ç¬¬ä¸€ä¸ªå®é™…çš„é¡µé¢
        page = context.pages[0] if context.pages else await context.new_page()
        browser = None
    else:
        storage_state = str(Config.STORAGE_STATE_FILE.absolute()) if Config.STORAGE_STATE_FILE.exists() else None
        if storage_state:
            print(f"Found session file at {Config.STORAGE_STATE_FILE}, attempting to reuse it.")
        else:
            print("No local session file found, proceeding with a new session (may require login).")

        browser = await playwright.chromium.launch(headless=False)
        context = await browser.new_context(
            storage_state=storage_state,
            user_agent=Config.USER_AGENT
        )
        await context.add_init_script(INIT_SCRIPT)
        page = await context.new_page()
    return browser, page, context

async def run(cats, playwright: Playwright, mode, remote_config, catch_num, catch_per_minute):
    browser, page, context = await get_chrome(playwright, mode, remote_config)
    try:
        print(f"Navigating to rank page: {Config.RANK_URL}")
        await page.goto(Config.RANK_URL, wait_until="domcontentloaded")

        # å¦‚æœæ‚¨åœ¨æµè§ˆå™¨ä¸­å·²ç»ç™»å½•ï¼Œåˆ™å¯èƒ½ä¸éœ€è¦æ­¤ç™»å½•æ£€æŸ¥
        if "login" in page.url:
            print("Login required. Please log in in the browser window.")
            print("Waiting for successful login...")
            await page.wait_for_url(lambda url: "login" not in url, timeout=Config.LOGIN_TIMEOUT)
            print("Login successful. Continuing script.")

        await page.wait_for_timeout(random.randint(1000, 2500))
        print("Clicking 'è¶‹åŠ¿æ¦œ' (Trend Rank)...")
        await page.get_by_text("è¶‹åŠ¿æ¦œ", exact=True).click()

        await page.wait_for_timeout(random.randint(1000, 2500))

        # é€‰æ‹©è¿‡æ»¤æ¡ä»¶
        print("Clicking 'çŸ­è§†é¢‘' (Short Video) and waiting for rank data...")
        await page.locator("div").filter(has_text=re.compile(r"^ä½“éªŒåˆ†$")).click()
        await page.get_by_role("menuitem", name="â‰¥85").click()
        await page.get_by_text("çŸ­è§†é¢‘", exact=True).click()

        # æ–°å¼€ä¸€ä¸ªç•Œé¢ï¼Œç»™è¯¦æƒ…ç”¨
        page_detail = await context.new_page()
        # å¾ªç¯é€‰æ‹©ç±»ç›®ï¼Œè§¦å‘åŠ è½½
        for cat in cats:
            print("è§¦å‘ç±»ç›®", cat)
            try:
                data_list = await cat_run(page, page_detail, cat, catch_num, catch_per_minute)
            except Exception as e:
                continue
    except TimeoutError:
        print("âŒ Operation timed out. Please check your network or if the page structure has changed.")
    except Exception as e:
        print(f"âŒ An unexpected error occurred: {e}")
    finally:
        print("Script finished. Closing browser context.")
        if mode != "remote":
            print("Saving current session state (cookies, etc.)...")
            await context.storage_state(path=Config.STORAGE_STATE_FILE)
            print(f"Session state saved to: {Config.STORAGE_STATE_FILE}")
            await context.close()
            await browser.close()
            print("Browser closed.")
        else:
            # å› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯æŒä¹…åŒ–ä¸Šä¸‹æ–‡ï¼Œæ‰€ä»¥ä¸éœ€è¦ä¿å­˜å­˜å‚¨çŠ¶æ€ã€‚
            # æˆ‘ä»¬åªå…³é—­ä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæµè§ˆå™¨ã€‚
            await context.close()
            print("Browser context closed.")


async def main():
    user_data_dir = r"C:\Users\gsma\AppData\Local\Google\Chrome\User Data"
    executable_path = r"C:\Users\gsma\AppData\Local\Google\Chrome\Application\chrome.exe"
    # æ¯ä¸ªæ¦œå•æŠ“å–çš„æ•°æ®é‡
    catch_num = 100
    # æ²¡åˆ†é’ŸæŠ“å‡ ä¸ª
    catch_per_minute = 0.3
    cats = [
        # "æœé¥°å†…è¡£",
        # "ç¾å¦†",
        # "é£Ÿå“é¥®æ–™",
        # ä¸€ä¸ªè´¦å·ä¸€ä¸ªï¼Œç„¶å2åˆ†é’Ÿä¸€æ¬¡è¯·æ±‚
        "ä¸ªæŠ¤å®¶æ¸…",
        ## "é‹é´ç®±åŒ…",
        # "é’Ÿè¡¨é…é¥°",
        ## "æ¯å©´å® ç‰©",
        # "å›¾ä¹¦æ•™è‚²",
        # "æ™ºèƒ½å®¶å±…",
        # "3Cæ•°ç äº§å“",
        # "è¿åŠ¨æˆ·å¤–",
        # "ç©å…·ä¹å™¨",
        # "ç”Ÿé²œ"
    ]
    async with async_playwright() as playwright:
        await run(cats, playwright, "remote", {
            "user_data_dir": user_data_dir,
            "executable_path": executable_path,
        }, catch_num, catch_per_minute)


if __name__ == "__main__":
    print("Starting Playwright data scraping script...")
    try:
        import playwright
    except ImportError:
        print("Error: 'playwright' library not found.")
        print("Please install it via pip: pip install playwright")
        print("And then run: playwright install")
        exit(1)

    asyncio.run(main())
